<!doctype linuxdoc system>

<!-- Here's an SGML example file. Format it and print out the source, and
     use it as a model for your own SGML files. As you can see this is a
     comment.
 -->

<article>

<!-- Title information -->

<title>Happy User Guide
<author>Simon Marlow <tt/simonm@dcs.gla.ac.uk/
<date>11 December 1997
<abstract>
This document describes Happy, the Haskell Parser Generator, version
1.5.
</abstract>

<!-- Table of contents -->
<toc>

<!-- Begin the document -->

<sect> Introduction
<label id="sec:introduction">
<p>

Happy is a parser generator system for Haskell, similar to the tool
<tt/yacc/ for C.  Like <tt/yacc/, it takes a file containing an
annotated BNF specification of a grammar and produces a Haskell module
containing a parser for the grammar.

Happy is flexible; unlike <tt/yacc/, you can have several Happy
parsers in the same program.  Happy can work in conjunction with a
lexical analyser supplied by the user (either hand-written or
generated by another program), or it can parse a stream of characters
directly (but this isn't practical in most cases).  In a future
version we hope to include a lexical analyser generator with Happy as
a single package.

Parsers generated by Happy are fast; generally faster than an
equivalent parser written using parsing combinators or similar tools.
Furthermore, any future improvements made to Happy will benefit an
existing grammar, without need for a rewrite.

Happy is sufficiently powerful to parse Haskell itself - there's a
freely available Haskell parser written using Happy which can be
obtained from <url
url="http://www.dcs.gla.ac.uk/~simonm/hsparse.tar.gz">.

Happy can currently generate three types of parser from a given grammar,
the intention being that we can experiment with different kinds of
functional code to see which is the best, and compiler writers can use
the different types of parser to tune their compilers.  The types of
parser supported are:

<enum>

<item> `standard' Haskell 1.4 (should work with any compiler that
compiles Haskell 1.4).

<item> standard Haskell using arrays (this is not the default because
we have found this generates slower parsers than 1).

<item> Haskell with GHC (Glasgow Haskell) extensions.  This is a
slightly faster option than 1 for Glasgow Haskell users.

</enum>

<sect1> Compatibility
<label id="sec:compatibility">
<p>

Happy is written in Haskell 1.4. This means that it will compile under
Glasgow Haskell (recommended), hbc, Hugs, and any other Haskell 1.4
compiler.

Remember: parsers produced using Happy should compile happily (sic)
under any Haskell 1.4 complier.  A compatibility mode enables Happy to
generate parsers that will compile with a Haskell 1.2 compiler, such
as GHC 0.29.

<sect1> Reporting Bugs
<label id="sec:reporting-bugs">
<p>

Any bugs found in Happy should be reported to me (Simon Marlow
<htmlurl url="mailto:simonm@dcs.gla.ac.uk"
name="simonm@dcs.gla.ac.uk">) including all the relevant information:
- the compiler used to compile Happy, the command-line options used,
your grammar file or a cut-down example showing the problem, and a
description of what goes wrong.  A patch to fix the problem would also
be greatly appreciated.

Requests for new features should also be sent to the above address,
especially if accompanied by patches.  You never know, if I get
another request to implement yacc-style precedences, I might just do
it :-)

<sect1> License
<label id="sec:license">
<p>

Previous versions of Happy were covered by the GNU general public
license.  We're now distributing Happy with a less restrictive
license:

Happy may be freely distributed in source or binary form, provided:

<itemize>
<item> Source distributions must retain the enclosed <tt/LICENSE/
file, and copyright attributions on each source file.  

<item> Binary distributions must contain the <tt/LICENSE/ file either
as part of the distribution or in accompanying electronic or printed
material.
</itemize>

Aside from these restrictions, you can do what you like with Happy.

<sect1> Obtaining Happy
<label id="sec:obtaining">
<p>

The primary location of Happy is <url
url="ftp://ftp.dcs.gla.ac.uk/pub/haskell/happy">.  The following sites
mirror this directory:

<itemize>
<item> <url url="ftp://src.doc.ic.ac.uk/computing/programming/languages/haskell/happy">
<item> <url url="ftp://cs.chalmers.se/pub/haskell/(somewhere)">
<item> <url url="ftp://cs.yale.edu/pub/haskell/(somewhere)">
</itemize>

Happy is also distributed along with the Glasgow Haskell Compiler.  A
World Wide Web page describing Happy can be found at <url
url="http://www.dcs.gla.ac.uk/fp/software/happy.html">.

<sect> Using Happy
<label id="sec:using">
<p>

Users of Yacc will find Happy quite familiar.  The basic idea is as
follows:

<itemize>
<item> Define the grammar you want to parse in a Happy grammar file. 

<item> Run the grammar through Happy, to generate a compilable Haskell
module.

<item> Use this module as part of your Haskell program, usually in
conjunction with a lexical analyser (a function that splits the input
into ``tokens'', the basic unit of parsing).
</itemize>

Let's run through an example.  We'll implement a parser for a simple
expression syntax, consisting of integers, variables, the operators
<tt/+/, <tt/-/, <tt/*/, <tt>/</tt>, and the form <tt/let var = exp in
exp/.  The grammar file starts off like this:

<tscreen> <verb>
{
module Main where
}
</verb> </tscreen>

At the top of the file is an optional module header, which is just a
Haskell module header enclosed in braces.  This code is emitted
verbatim into the generated module, so you can put any Haskell code
here at all.  In a grammar file, Haskell code is always contained
between curly braces to distinguish it from the grammar.

In this case, the parser will be a standalone program so we'll call
the module <tt/Main/.

Next comes a couple of declarations:

<tscreen> <verb>
%name calc
%tokentype { Token }
</verb> </tscreen>

The first line declares the name of the parsing function that Happy
will generate, in this case <tt/calc/.  In many cases, this is the
only symbol you need to export from the module.

The second line declares the type of tokens that the parser will
accept.  The parser (i.e. the function <tt/calc/) will be of type
<tt/[Token] -> T/, where <tt/T/ is the return type of the parser,
determined by the production rules below.

Now we declare all the possible tokens:

<tscreen> <verb>
%token 
      let             { TokenLet }
      in              { TokenIn }
      int             { TokenInt $$ }
      var             { TokenVar $$ }
      '='             { TokenEq }
      '+'             { TokenPlus }
      '-'             { TokenMinus }
      '*'             { TokenTimes }
      '/'             { TokenDiv }
      '('             { TokenOB }
      ')'             { TokenCB }
</verb> </tscreen>

The symbols on the left are the tokens as they will be referred to in
the rest of the grammar, and to the right of each token enclosed in
braces is a Haskell pattern that matches the token.  The parser will
expect to receive a stream of tokens, each of which will match one of
the given patterns (the definition of the <tt/Token/ datatype is given
later).

The <tt/&dollar;&dollar;/ symbol is a placeholder that represents the
<em/value/ of this token.  Normally the value of a token is the token
itself, but by using the <tt/&dollar;&dollar;/ symbol you can specify
some component of the token object to be the value.

Like yacc, we include <tt/%%/ here, for no real reason.

<tscreen> <verb>
%%
</verb> </tscreen>

Now we have the production rules for the grammar.

<tscreen> <verb>
Exp   : let var '=' Exp in Exp  { Let $2 $4 $6 }
      | Exp1                    { Exp1 $1 }

Exp1  : Exp1 '+' Term           { Plus $1 $3 }
      | Exp1 '-' Term           { Minus $1 $3 }
      | Term                    { Term $1 }

Term  : Term '*' Factor         { Times $1 $3 }
      | Term '/' Factor         { Div $1 $3 }
      | Factor                  { Factor $1 }

Factor			  
      : int                     { Int $1 }
      | var                     { Var $1 }
      | '(' Exp ')'             { Brack $2 }
</verb> </tscreen>

Each production consists of a <em/non-terminal/ symbol on the left,
followed by a colon, followed by one or more expansions on the right,
separated by <tt/|/.  Each expansion has some Haskell code associated
with it, enclosed in braces as usual.  

The way to think about a parser is with each symbol having a `value':
we defined the values of the tokens above, and the grammar defines the
values of non-terminal symbols in terms of sequences of other symbols
(either tokens or non-terminals).  In a production like this:

<tscreen> <verb>
n   : t_1 ... t_n   { E }
</verb> </tscreen>

whenever the parser finds the symbols <tt/t_1..t_n/ in the token
stream, it constructs the symbol <tt/n/ and gives it the value <tt/E/,
which may refer to the values of <tt/t_1...t_n/ using the symbols
<tt/&dollar;1...&dollar;n/.

The parser reduces the input using the rules in the grammar until just
one symbol remains: the first symbol defined in the grammar (namely
<tt/Exp/ in our example).  The value of this symbol is the return
value from the parser.

To complete the program, we need some extra code.  The grammar file
may optionally contain a final code section, enclosed in curly braces.

<tscreen> <verb>
{
</verb> </tscreen>

All parsers must declare the function <tt/happyError/, which is called
when an error is detected.

<tscreen> <verb>
happyError :: [Token] -> a
happyError _ = error "Parse error"
</verb> </tscreen>

<tt/happyError/ doesn't have to call <tt/error/: it can return a real
value, which must be the same type as the return value of the parser
itself.  A convenient way to deal with errors is to use an exception
monad, which we'll talk about in Section <ref id="sec:monads">.  It's
also possible to keep track of line numbers in the parser for use in
error messages, this is described in Section <ref
id="sec:line-numbers">.

Next we can declare the data type that represents the parsed
expression:

<tscreen> <verb>
data Exp  
      = Let String Exp Exp
      | Exp1 Exp1

data Exp1 
      = Plus Exp1 Term 
      | Minus Exp1 Term 
      | Term Term

data Term 
      = Times Term Factor 
      | Div Term Factor 
      | Factor Factor

data Factor 
      = Int Int 
      | Var String 
      | Brack Exp
</verb> </tscreen>

And the data structure for the tokens...

<tscreen> <verb>
data Token
      = TokenLet
      | TokenIn
      | TokenInt Int
      | TokenVar String
      | TokenEq
      | TokenPlus
      | TokenMinus
      | TokenTimes
      | TokenDiv
      | TokenOB
      | TokenCB
 deriving Show
</verb> </tscreen>

... and a simple lexer that returns this data structure.

<tscreen> <verb>
lexer :: String -> [Token]
lexer [] = []
lexer (c:cs) 
      | isSpace c = lexer cs
      | isAlpha c = lexVar (c:cs)
      | isDigit c = lexNum (c:cs)
lexer ('=':cs) = TokenEq : lexer cs
lexer ('+':cs) = TokenPlus : lexer cs
lexer ('-':cs) = TokenMinus : lexer cs
lexer ('*':cs) = TokenTimes : lexer cs
lexer ('/':cs) = TokenDiv : lexer cs
lexer ('(':cs) = TokenOB : lexer cs
lexer (')':cs) = TokenCB : lexer cs

lexNum cs = TokenInt (read num) : lexer rest
      where (num,rest) = span isDigit cs

lexVar cs =
   case span isAlpha cs of
      ("let",rest) -> TokenLet : lexer rest
      ("in",rest)  -> TokenIn : lexer rest
      (var,rest)   -> TokenVar var : lexer rest
</verb> </tscreen>

And finally a top-level function to take some input, parse it, and
print out the result.

<tscreen> <verb>
main = getContents >>= print . calc . lexer
}
</verb> </tscreen>

And that's it! A whole lexer, parser and grammar in a few dozen lines.
Another good example is Happy's own parser. Several features in Happy
were developed using this as an example.

To generate the Haskell module for this parser, type the command
<tt/happy example.ly/ (where <tt/example.ly/ is the name of the
grammar file).  The Haskell module will be placed in a file named
<tt/example.hs/.  Additionally, invoking the command <tt/happy
example.ly -i/ will produce the file <tt/example.info/ which contains
detailed information about the parser, including states and reduction
rules (see Section <ref id="sec:info-files">).  This can be invaluable
for debugging parsers, but requires some knowledge of the operation of
a shift-reduce parser.

<sect1> Returning other datatypes
<label id="sec:other-datatypes">
<p>

In the above example, we used a data type to represent the syntax
being parsed.  However, there's no reason why it has to be this way:
you could calculate the value of the expression on the fly, using
productions like this:

<tscreen> <verb>
Term  : Term '*' Factor         { $1 * $3 }
      | Term '/' Factor         { $1 / $3 }
      | Factor                  { $1 }
</verb> </tscreen>

The value of a <tt/Term/ would be the value of the expression itself,
and the parser could return an integer.  

This works for simple expression types, but our grammar includes
variables and the <tt/let/ syntax.  How do we know the value of a
variable while we're parsing it?  We don't, but since the Haskell code
for a production can be anything at all, we could make it a function
that takes an environment of variable values, and returns the computed
value of the expression:

<tscreen> <verb>
Exp   : let var '=' Exp in Exp  { \p -> $6 (($2,$4 p):p) }
      | Exp1                    { $1 }

Exp1  : Exp1 '+' Term           { \p -> $1 p + $3 p }
      | Exp1 '-' Term           { \p -> $1 p - $3 p }
      | Term                    { $1 }

Term  : Term '*' Factor         { \p -> $1 p * $3 p }
      | Term '/' Factor         { \p -> $1 p `div` $3 p }
      | Factor                  { $1 }

Factor			  
      : int                     { \p -> $1 }
      | var                     { \p -> case lookup $1 p of
	                                    Nothing -> error "no var"
					    Just i  -> i }
      | '(' Exp ')'             { $2 }
</verb> </tscreen>

The value of each production is a function from an environment
<em/p/ to a value.  When parsing a <tt/let/ construct, we extend the
environment with the new binding to find the value of the body, and
the rule for <tt/var/ looks up its value in the environment.  There's
something you can't do in <tt/yacc/ :-)

<sect1> Parsing sequences
<label id="sec:sequences">
<p>

A common feature in grammars is a <em/sequence/ of a particular
syntactic element.  In EBNF, we'd write something like <tt/n+/ to
represent a sequence of one or more <tt/n/s, and <tt/n*/ for zero or
more.  Happy doesn't support this syntax explicitly, but you can
define the equivalent sequences using simple productions.

For example, the grammar for Happy itself contains a rule like this:

<tscreen> <verb>
prods : prod                   { [$1] }
      | prods prod             { $2 : $1 }
</verb> </tscreen>

In other words, a sequence of productions is either a single
production, or a sequence of productions followed by a single
production.  This recursive rule defines a sequence of one or more
productions.

One thing to note about this rule is that we used <em/left recursion/
to define it - we could have written it like this:

<tscreen> <verb>
prods : prod                  { [$1] }
      | prod prods            { $1 : $2 }
</verb> </tscreen>

The only reason we used left recursion is that Happy is more efficient
at parsing left-recursive rules; they result in a constant stack-space
parser, whereas right-recursive rules require stack space proportional
to the length of the list being parsed.  This can be extremely
important where long sequences are involved, for instance in
automatically generated output.  For example, the parser in GHC used
to use right-recursion to parse lists, and as a result it failed to
parse some Happy-generated modules due to running out of stack space!

One implication of using left recursion is that the resulting list
comes out reversed, and you have to reverse it again to get it in the
original order.  Take a look at the Happy grammar for Haskell for many
examples of this.

Parsing sequences of zero or more elements requires a trivial change
to the above pattern:

<tscreen> <verb>
prods : {- empty -}           { [] }
      | prods prod            { $2 : $1 }
</verb> </tscreen>

Yes - empty productions are allowed.  The normal convention is to
include the comment <tt/{- empty -}/ to make it more obvious to a
reader of the code what's going on.

<sect2> Sequences with separators
<label id="sec:separators">
<p>

A common type of sequence is one with a <em/separator/: for instance
function bodies in C consist of statements separated by semicolons.
To parse this kind of sequence we use a production like this:

<tscreen> <verb>
stmts : stmt                   { [$1] }
      | stmts ';' stmt         { $3 : $1 }
</verb> </tscreen>

If the <tt/;/ is to be a <em/terminator/ rather than a separator
(i.e. there should be one following each statement), we can remove the
semicolon from the above rule and redefine <tt/stmt/ as

<tscreen> <verb>
stmt : stmt1 ';'              { $1 }
</verb> </tscreen>

where <tt/stmt1/ is the real definition of statements.

We might like to allow extra semicolons between statements, to be a
bit more liberal in what we allow as legal syntax.  We probably just
want the parser to ignore these extra semicolons, and not generate a
``null statement'' value or something.  The following rule parses a
sequence or zero or more statements separated by semicolons, in which
the statements may be empty:

<tscreen> <verb>
stmts : stmts ';' stmt          { $3 : $1 }
      | stmts ';'               { $1 }
      | stmt			{ [$1] }
      | {- empty -}		{ [] }
</verb> </tscreen>

Parsing sequences of <em/one/ or more possibly null statements is left
as an exercise for the reader...

<sect1> Ambiguities
<label id="sec:ambiguities">
<p>

(section under construction)

<sect1> Type Signatures
<label id="sec:type-signatures">
<p>

Happy allows you to include type signatures in the grammar file
itself, to indicate the type of each production.  This has several
benefits:

<itemize>
<item> Documentation: including types in the grammar helps to document
the grammar for someone else (and indeed yourself) reading the code.
<item> Fixing type errors in the generated module can become slightly
easier if Happy has inserted type signatures for you.  This is a
slightly dubious benefit, since type errors in the generated module
are still somewhat difficult to find.
<item> Type signatures generally help the Haskell compiler to compile the
parser faster.  This is important when really large grammar files are
being used.
</itemize>

The syntax for type signatures in the grammar file is as follows:

<tscreen> <verb>
stmts   :: { [ Stmt ] }
stmts   : stmts stmt                { $2 : $1 }
	| stmt                      { [$1] }
</verb> </tscreen>

In fact, you can leave out the superfluous occurrence of <tt/stmts/:

<tscreen> <verb>
stmts   :: { [ Stmt ] }
	: stmts stmt                { $2 : $1 }
	| stmt                      { [$1] }
</verb> </tscreen>

Note that currently, you have to include type signatures for <em/all/
the productions in the grammar to benefit from the second and third
points above.  This is due to boring technical reasons, but it is
hoped that this restriction can be removed in the future.

<sect1> Monadic Parsers
<label id="sec:monads">
<p>

Happy has support for threading a monad through the generated parser.
This might be useful for several reasons:

<itemize>

<item> Handling parse errors by using an exception monad (see Section
<ref id="sec:exception" name="Handling Parse Errors">).

<item> Keeping track of line numbers in the input file, for example
for use in error messages (see Section <ref id="sec:line-numbers"
name="Line Numbers">).

<item> Performing IO operations during parsing.

<item> Parsing languages with context-dependencies (such as C) require
some state in the parser.

</itemize>

Adding monadic support to your parser couldn't be simpler.  Just add
the following directive to the declaration section of the grammar
file:

<tscreen> <verb>
%monad { <type> } [ { <then> } { <return> } ]
</verb> </tscreen>

where <tt/&lt;type&gt;/ is the type constructor for the monad,
<tt/&lt;then&gt;/ is the bind operation of the monad, and
<tt/&lt;return&gt;/ is the return operation. If you leave out
the names for the bind and return operations, Happy assumes that 
<tt/&lt;type&gt;/ is an instance of the standard Haskell type
class <tt/Monad/ and uses the overloaded names for the bind and return
operations.

When this declaration is included in the grammar, Happy makes a couple
of changes to the generated parser: the types of the main parser
function and <tt/happyError/ become <tt/[Token] -&gt; P a/ where
<tt/P/ is the monad type constructor, and <tt/a/ is the type of the
top production.  In other words, Happy adds an application of the
<tt/&lt;return&gt;/ operation defined in the declaration above, around
the result of the parser (<tt/happyError/ is affected because it must
have the same return type as the parser).  And that's all it does.

This still isn't very useful: all you can do is return something of
monadic type from <tt/happyError/.  How do you specify that the
productions can also have type <tt/P a/?  Most of the time, you don't
want a production to have this type: you'd have to write explicit
<tt/returnP/s everywhere.  However, there may be a few rules in a
grammar that need to get at the monad, so Happy has a special syntax
for monadic productions:

<tscreen> <verb>
n  :  t_1 ... t_n          {% <expr> }
</verb> </tscreen>

The <tt/%/ in the action indicates that this is a monadic action, with
type <tt/P a/, where <tt/a/ is the real return type of the
production.  When Happy reduces one of these rules, it evaluates the
expression 

<tscreen> <verb>
<expr> `then` \result -> <continue parsing>
</verb> </tscreen>

Happy uses <tt/result/ as the real semantic value of the production.
During parsing, several monadic actions might be reduced, resulting in
a sequence like

<tscreen> <verb>
<expr1> `then` \r1 ->
<expr2> `then` \r2 ->
...
return <expr3>
</verb> </tscreen>

The monadic actions are performed in the order that they are
<em/reduced/.  If we consider the parse as a tree, then reductions
happen in a depth-first left-to-right manner.  The great thing about
adding a monad to your parser is that it doesn't impose any
performance overhead for normal reductions - only the monadic ones are
translated like this.

Take a look at the Haskell parser for a good illustration of how to
use a monad in your parser: it contains examples of all the principles
discussed in this section, namely parse errors, a threaded lexer,
line/column numbers, and state communication between the parser and
lexer.

The following sections consider a couple of uses for monadic parsers,
and describe how to also thread the monad through the lexical analyser.

<sect2> Handling Parse Errors
<label id="sec:exception">
<p>

It's not very convenient to just call <tt/error/ when a parse error is
detected: in a robust setting, you'd like the program to recover
gracefully and report a useful error message to the user.  Exceptions
(of which errors are a special case) are normally implemented in
Haskell by using an exception monad, something like:

<tscreen> <verb>
data E a = Ok a | Failed String

thenE :: E a -> (a -> E b) -> E b
m `thenE` k = 
   case m of 
       Ok a -> k a
	 Failed e -> Failed e

returnE :: a -> E a
returnE a = Ok a

failE :: String -> E a
failE err = Failed err

catchE :: E a -> (String -> E a) -> E a
catchE m k = 
   case m of
      Ok a -> OK a
	Failed e -> k e
</verb> </tscreen>

This monad just uses a string as the error type.  The functions
<tt/thenE/ and <tt/returnE/ are the usual bind and return operations
of the monad, <tt/failE/ raises an error, and <tt/catchE/ is a
combinator for handling exceptions.

We can add this monad to the parser with the declaration

<tscreen> <verb>
%monad { E } { thenE } { returnE }
</verb> </tscreen>

Now, without changing the grammar, we can change the definition of
<tt/happyError/ and have something sensible happen for a parse error:

<tscreen> <verb>
happyError tokens = failE "Parse error"
</verb> </tscreen>

The parser now raises an exception in the monad instead of bombing out
on a parse error.

We can also generate errors during parsing.  There are times when it
is more convenient to parse a more general language than that which is
actually intended, and check it later.  An example comes from Haskell,
where the precedence values in infix declarations must be between 0
and 9:

<tscreen> <verb>
prec :: { Int }
      : int    {% if $1 < 0 || $1 > 9 
	                then failE "Precedence out of range"
		        else returnE $1
		}
</verb> </tscreen>

The monadic action allows the check to be placed in the parser itself,
where it belongs.

<sect2> Threaded Lexers
<label id="sec:lexers">
<p>

Happy allows the monad concept to be extended to the lexical analyser,
too.  This has several useful consequences:

<itemize>
<item> Lexical errors can be treated in the same way as parse errors,
using an exception monad.
<item> Information such as the current file and line number can be
communicated between the lexer and parser.
<item> General state communication between the parser and lexer - for
example, implementation of the Haskell layout rule requires this kind
of interaction.
<item> IO operations can be performed in the lexer - this could be
useful for following import/include declarations for instance.
</itemize>

A monadic lexer is requested by adding the following declaration to
the grammar file:

<tscreen> <verb>
%lexer { <lexer> } { <eof> }
</verb> </tscreen>

where <tt/&lt;lexer&gt;/ is the name of the lexical analyser function,
and <tt/&lt;eof&gt;/ is a token that is to be treated as the end of
file.

When using a monadic lexer, the parser no longer reads a list of
tokens.  Instead, it calls the lexical analysis function for each new
token to be read.  This has the side effect of eliminating the
intermediate list of tokens, which is a slight performance win.

The type of the main parser function and <tt/happyError/ is now just
<tt/P a/ - the input is being handled completely within the monad.

The lexical analysis function must have the following type:

<tscreen> <verb>
lexer :: (Token -> P a) -> P a
</verb> </tscreen>

where <tt/P/ is the monad type constructor declared with <tt/%monad/,
and <tt/a/ can be replaced by the parser return type if desired.

You can see from this type that the lexer takes a <em/continuation/ as
an argument.  The lexer is to find the next token, and pass it to this
continuation to carry on with the parse.  Obviously, we need to keep
track of the input in the monad somehow, so that the lexer can do
something different each time it's called!

Let's take the exception monad above, and extend it to add the input
string so that we can use it with a threaded lexer.

<tscreen> <verb>
data ParseResult a = Ok a | Failed String
type P a = String -> ParseResult a

thenP :: P a -> (a -> P b) -> P b
m `thenP` k = \s ->
   case m s of 
       Ok a -> k a s
	 Failed e -> Failed e

returnP :: a -> P a
returnP a = \s -> Ok a

failP :: String -> P a
failP err = \s -> Failed err

catchP :: P a -> (String -> P a) -> P a
catchP m k = \s ->
   case m s of
      Ok a -> OK a
	Failed e -> k e s
</verb> </tscreen>

Notice that this isn't a real state monad - the input string just gets
passed around, not returned.  Our lexer will now look something like
this:

<tscreen> <verb>
lexer :: (Token -> P a) -> P a
lexer cont s = 
    ... lexical analysis code ...
    cont token s'
</verb> </tscreen>

the lexer grabs the continuation and the input string, finds the next
token <tt/token/, and passes it together with the remaining input
string <tt/s'/ to the continuation.

We can now indicate lexical errors by ignoring the continuation and
calling <tt/failP "error message" s/ within the lexer (don't forget to
pass the input string to make the types work out).

This may all seem a bit weird.  Why, you ask, doesn't the lexer just
have type <tt/P Token/?  Well, the decision was taken to use a
continuation purely for performance reasons.  If the lexer must return
a token, then it turns out that the input string must be a real state
object, and the monad has to return it as well as pass it around.  If
you want a lexer of type <tt/P Token/, then just define a wrapper to
deal with the continuation:

<tscreen> <verb>
lexwrap :: (Token -> P a) -> P a
lexwrap cont = real_lexer `thenP` \token -> cont token
</verb> </tscreen>

<sect2> Line Numbers
<label id="sec:line-numbers">
<p>

Previous versions of Happy had a <tt/%newline/ directive that enabled
simple line numbers to be counted by the parser and referenced in the
actions.  We warned you that this facility may go away and be replaced
by something more general, well guess what? :-)

Line numbers can now be dealt with quite straightforwardly using a
monadic parser/lexer combination.  Ok, we have to extend the monad a
bit more:

<tscreen> <verb>
type LineNumber = Int
type P a = String -> LineNumber -> ParseResult a

getLineNo :: P LineNumber
getLineNo = \s l -> Ok l
</verb> </tscreen>

(the rest of the functions in the monad follow by just adding the
extra line number argument in the same way as the input string).
Again, the line number is just passed down, not returned: this is OK
because of the continuation-based lexer that can change the line
number and pass the new one to the continuation.

The lexer can now update the line number as follows:

<tscreen> <verb>
lexer cont s =
  case s of
     '\n':s  ->  \line -> lexer cont s (line + 1)
     ... rest of lexical analysis ...
</verb> </tscreen>

It's as simple as that.  Take a look at Happy's own parser if you have
the sources lying around, it uses a monad just like the one above.

Reporting the line number of a parse error is achieved by changing
<tt/happyError/ to look something like this:

<tscreen> <verb>
happyError :: P a
happyError = getLineNo `thenP` \line -> 
             failP (show line ++ ": parse error")
</verb> </tscreen>

We can also get hold of the line number during parsing, to put it in
the parsed data structure for future reference.  A good way to do this
is to have a production in the grammar that returns the current line
number: 

<tscreen> <verb>
lineno :: { LineNumber }
        : {- empty -}      {% getLineNo }
</verb> </tscreen>

The semantic value of <tt/lineno/ is the line number of the last token
read - this will always be the token directly following the
<tt/lineno/ symbol in the grammar, since Happy always keeps one
lookahead token in reserve.

<sect2> Summary
<label id="sec:monad-summary">
<p>

The types of various functions related to the parser are dependent on
what combination of <tt/%monad/ and <tt/%lexer/ directives are present
in the grammar.  For reference, we list those types here.

<descrip>
<tag> No <tt/&percnt;monad/ or <tt/&percnt;lexer/. </tag>
<tscreen> <verb>
parse      :: [Token] -> a
happyError :: [Token] -> a
</verb> </tscreen>

<tag> with <tt/%monad/. </tag>
<tscreen> <verb>
parse      :: [Token] -> P a
happyError :: [Token] -> P a
</verb> </tscreen>

<tag> with <tt/%lexer/. </tag>
<tscreen> <verb>
parse      :: T a
happyError :: T a
lexer      :: (Token -> T a) -> T a
</verb> </tscreen>
where the type constructor <tt/T/ is whatever you want (usually <tt/T
a = String -> a/.  I'm not sure if this is useful, or even if it works
properly.

<tag> with <tt/%monad/ and <tt/%lexer/. </tag>
<tscreen> <verb>
parse      :: P a
happyError :: P a
lexer      :: (Token -> P a) -> P a
</verb> </tscreen>

</descrip>

<sect1> The Error Token
<label id="sec:error">
<p>

Happy supports a limited form of error recovery, using the special
symbol <tt/error/ in a grammar file.  When Happy finds a parse error
during parsing, it automatically inserts the <tt/error/ symbol; if
your grammar deals with <tt/error/ explicitly, then it can detect the
error and carry on.

For example, the Happy grammar for Haskell uses error recovery to
implement Haskell layout.  The grammar has a rule that looks like
this:

<tscreen> <verb>
close : '}'                  { () }
      | error		     { () }
</verb> </tscreen>

This says that a close brace in a layout-indented context may be
either a curly brace (inserted by the lexical analyser), or a parse
error.  

This rule is used to parse expressions like <tt/let x = e in e'/: the
layout system inserts an open brace before <tt/x/, and the occurrence
of the <tt/in/ symbol generates a parse error, which is interpreted
as a close brace by the above rule.

Note for <tt/yacc/ users: this form of error recovery is strictly more
limited than that provided by <tt/yacc/.  During a parse error
condition, <tt/yacc/ attempts to discard states and tokens in order to
get back into a state where parsing may continue; Happy doesn't do
this.  The reason is that normal <tt/yacc/ error recovery is
notoriously hard to describe, and the semantics depend heavily on the
workings of a shift-reduce parser.  Furthermore, different
implementations of <tt/yacc/ appear to implement error recovery
differently.  Happy's limited error recovery on the other hand is
well-defined, as is just sufficient to implement the Haskell layout
rule (which is why it was added in the first place).

<sect> Invoking Happy
<label id="sec:invoking">
<p>

An invocation of Happy has the following syntax:

<tscreen> <verb>
happy [ options ] <filename> [ options ]
</verb> </tscreen>

All the command line options are optional (!) and may occur either
before or after the input file name.

There are two types of grammar files, <tt/file.y/ and <tt/file.ly/,
with the latter observing the reverse comment bird track convention
(i.e. each code line must begin with '>').  The examples distributed
with Happy are all of the .ly form.

Caveat: When using hbc (Chalmers Haskell) the command argument structure
is slightly different. This is because the hbc run time system takes
some flags as its own (for setting things like the heap size, etc).
This problem can be circumvented by adding a single @code{-} to your
command line.  So when using a hbc generated version of Happy, the
argument structure is:

<tscreen> <verb>
happy - [ options ] <filename> [ options ]
</verb> </tscreen>

Pedantic programmers can either use aliases or a short shell wrapper
to automatically add the @code{-}, along with any required run time
system arguments (Or just use GHC to compile Happy!).

The flags accepted by Happy are as follows:

<descrip>

<tag> <tt/-a | --arrays/ </tag> <p>
Instructs Happy to generate a parser using an array-based shift reduce
parser.  Currently this generates slower parsers for two reasons:
standard Haskell arrays are surrounded by overloading and
bounds-checking which makes lookup quite expensive, and secondly current
compilers don't know how to generate static objects (i.e. they generate
the code which builds the array at run-time).

<tag> <tt/-g | --ghc/ </tag> <p>
Instructs Happy to generate a parser that uses ghc-specific extensions
to obtain faster code.  When compiling the resulting Haskell module,
remember to use ghc with the <tt/-fglasgow-exts/ option, and to add
the declaration <tt/import GlaExts/ to the module header.

<tag> <tt/-i [ &lt;filename&gt; ] | --info [ &lt;filename&gt; ]/ </tag> <p>
Directs Happy to produce an info file containing detailed information
about the grammar, parser states, parser actions, and conflicts.  Info
files are vital during the debugging of grammars.  The filename
argument is optional, and if omitted the info file will be written to
<tt/&lt;file&gt;.info/ (where <tt/&lt;file&gt/; is the input file name
with any extension removed). Because this filename is optional, watch
out for:

<tscreen> <verb>
happy --info foo.ly
</verb> </tscreen>

which will use foo.ly as the name of the information file, and
complain that you have not specified a input file name.  This can be
solved by using:

<tscreen> <verb>
happy foo.ly --info
</verb> </tscreen>

<tag> <tt/-o &lt;filename&gt; | --outfile &lt;filename&gt;/ </tag> <p>
Specifies the destination of the generated parser module.  If omitted,
the parser will be placed in <tt/&lt;file&gt;.hs/, where <tt/&lt;file&gt;/
is the name of the input file with any extension removed.

<tag> <tt/-v | --verbose/ </tag>
Causes Happy to be more verbose.  At the moment this just prints out the
copyright message and has no effect on anything else.

<tag> <tt/-1.2/ </tag>
Instructs Happy to produce a Haskell 1.2-compatible module.  It may be
used in conjuction with any of the other options. This option will
almost certainly be removed in the future.

<tag> <tt/--template &lt;directory&gt;/ </tag>
Instructs Happy to use this directory when looking for the template
to including in the output.

</descrip>

Note: only one of the options <tt/-g/ and <tt/-a/ may be given (or
their long versions).  There is no array-based parser with ghc
extensions yet, but this is planned for a future version.

<sect> Syntax of Grammar Files
<label id="sec:grammar-files">
<p>

The input to Happy is a text file containing the grammar of the
language you want to parse, together with some annotations that help
the parser generator make a legal Haskell module that can be included
in your program.  This section gives the exact syntax of grammar
files. 

The overall format of the grammar file is given below:

<tscreen> <verb>
<optional module header>
<directives>
%%
<grammar>
<optional module trailer>
</verb> </tscreen>

If the name of the grammar file ends in <tt/.ly/, then it is assumed
to be a literate script.  All lines except those beginning with a
<tt/&gt/ will be ignored, and the <tt/&gt/ will be stripped from the
beginning of all the code lines.  There must be a blank line between
each code section (lines beginning with <tt/&gt/) and comment section.
Grammars not using the literate notation must be in a file with the
<tt/.y/ suffix.

<sect1> Lexical Rules
<label id="sec:lexical-rules">
<p>

Identifiers in Happy grammar files must take the following form (using
the BNF syntax from the Haskell Report):

<tscreen><verb> 
        id      ::= alpha { idchar }
                  | ' { any{^'} | \' } '
                  | " { any{^"} | \" } "

        alpha   ::= A | B | ... | Z
                  | a | b | ... | z

        idchar  ::= alpha
                  | 0 | 1 | ... | 9
                  | _
</verb></tscreen>

<sect1> Module Header
<label id="sec:module-header">
<p>

This section is optional, but if included takes the following form:

<tscreen><verb>
{  
<Haskell module header>
}
</verb></tscreen>

The Haskell module header contains the module name, exports, and
imports.  No other code is allowed in the header---this is because Happy
may need to include its own <tt/import/ statements directly after the
user defined header.

<sect1> Directives
<label id="sec:directives">
<p>

This section contains a number of lines of the form:

<tscreen><verb>
%<directive name> <argument> ...
</verb></tscreen>

The statements here are all annotations to help Happy generate the
Haskell code for the grammar.  Some of them are optional, and some of
them are required.

<sect2> Token Type
<label id="sec:token-type">
<p>

<tscreen><verb>
%tokentype   { <valid Haskell type> }
</verb></tscreen>

(mandatory) The <tt/%tokentype/ directive gives the type of the tokens
passed from the lexical analyser to the parser (in order that Happy
can supply types for functions and data in the generated parser).

<sect2> Tokens
<label id="sec:tokens">
<p>

<tscreen><verb>
%token <name> { <Haskell pattern> }
       <name> { <Haskell pattern> }
       ...
</verb></tscreen>

(mandatory) The <tt/%token/ directive is used to tell Happy about all
the terminal symbols used in the grammar.  Each terminal has a name,
by which it is referred to in the grammar itself, and a Haskell
representation enclosed in braces.  Each of the patterns must be of
the same type, given by the <tt/%tokentype/ directive.

The name of each terminal follows the lexical rules for Happy
identifiers given above.  There are no lexical differences between
terminals and non-terminals in the grammar, so it is recommended that
you stick to a convention; for example using upper case letters for
terminals and lower case for non-terminals, or vice-versa.

Happy will give you a warning if you try to use the same identifier both
as a non-terminal and a terminal, or introduce an identifier which is
declared as neither.

To save writing lots of projection functions that map tokens to their
components, you can include <tt/&dollar;&dollar;/ in your Haskell
pattern. For example:

<tscreen><verb>
%token INT { TokenInt $$ }
       ...
</verb></tscreen>

This makes the semantic value of <tt/INT/ refer to the first argument
of <tt/TokenInt/ rather than the whole token, eliminating the need for
any projection function.

<sect2> Parser Name
<label id="sec:parser-name">
<p>

<tscreen><verb>
%name <Haskell identifier>
</verb></tscreen>

(optional) The <tt/%name/ directive is followed by a valid Haskell
identifier, and gives the name of the top-level parsing function in
the generated parser.  This is the only function that needs to be
exported from a parser module.

If the parser name directive is omitted, it defaults to
<tt/happyParse/.

<sect2> Monad Directive
<label id="sec:monad-decl">
<p>

<tscreen><verb>
%monad { <type> } { <then> } { <return> }
</verb></tscreen>

(optional) The <tt/%monad/ directive takes three arguments: the type
constructor of the monad, the <tt/then/ (or <tt/bind/) operation, and
the <tt/return/ (or <tt/unit/) operation.  The type constructor can be
any type with kind <tt/* -> */.

Monad declarations are described in more detail in Section <ref
id="sec:monads" name="Monadic Parsers">.

<sect2> Lexical Analyser
<label id="sec:lexer-decl">
<p>

<tscreen><verb>
%lexer { <lexer> } { <eof> }
</verb></tscreen>

(optional) The <tt/%lexer/ directive takes two arguments:
<tt/&lt;lexer&gt;/ is the name of the lexical analyser function, and
<tt/&lt;eof&gt;/ is a token that is to be treated as the end of file.

Lexer declarations are described in more detail in Section <ref
id="sec:lexers" name="Threaded Lexers">.

<sect1> Grammar
<label id="sec:grammar">
<p>

The grammar section comes after the directives, separated from them by
a double-percent (<tt/%%/) symbol.  This section contains a number of
<em/productions/, each of which defines a single non-terminal.  Each
production has the following syntax:

<tscreen><verb>
<non-terminal> [ :: { <type> } ]
        :  <id> ... {[%] <expression> }
      [ |  <id> ... {[%] <expression> }
        ... ]
</verb></tscreen>

The first line gives the non-terminal to be defined by the production
and optionally its type (type signatures for productions are discussed
in Section <ref id="sec:type-signatures" name="Type Signatures">).

Each production has at least one, and possibly many right-hand sides.
Each right-hand side consists of zero or more symbols (terminals or
non-terminals) and a Haskell expression enclosed in braces.  The
expression represents the semantic value of the non-terminal, and may
refer to the semantic values of the symbols in the right-hand side
using the meta-variables <tt/&dollar;1 ... &dollar;n/.

Remember that all the expressions for a production must have the same
type.

A semantic value of the form <tt/{% ... }/ is a <em/monadic action/,
and is only valid when the grammar file contains a <tt/%monad/
directive (Section <ref id="sec:monad-decl" name="Monad Directive">).
Monadic actions are discussed in Section <ref id="sec:monads"
name="Monadic Parsers">.

<sect1> Module Trailer
<label id="sec:module-trailer">
<p>

The module trailer is optional, comes right at the end of the grammar
file, and takes the same form as the module header:

<tscreen><verb>
{
<Haskell code>
}
</verb></tscreen>

This section is used for placing auxiliary definitions that need to be
in the same module as the parser.  In small parsers, it often contains a
hand-written lexical analyser too.  There is no restriction on what can
be placed in the module trailer, and any code in there is copied
verbatim into the generated parser file.

<sect> Info Files
<label id="sec:info-files">
<p>

(section under construction)

<sect> Tips
<label id="sec:tips">
<p>

This section contains a lot of accumulated lore about using Happy.

<sect1> Performance Tips
<label id="sec:performance-tips">
<p>

How to make your parser go faster:

<itemize>

<item> If you are using ghc, generate parsers using the <tt/-g/
option, and compile them using ghc with the <tt/-fglasgow-exts/
option.  This is worth about 10-20%.

<item> The lexical analyser is usually the most performance critical
part of a parser, so it's worth spending some time optimising this.
Profiling tools are essential here.  In really dire circumstances,
resort to some of the hacks that are used in the Glasgow Haskell
Compiler's interface-file lexer.

<item> Simplify the grammar as much as possible, as this reduces the
number of states and reduction rules that need to be applied.

<item> Use left recursion rather than right recursion wherever possible.
While not strictly a performance issue, this affects the size of the
parser stack, which is kept on the heap and thus needs to be garbage
collected.

</itemize>


<sect1> Compilation-Time Tips
<label id="sec:compilation-time">
<p>

We have found that compiling parsers generated by Happy can take a
large amount of time/memory, so here's some tips on making things more
sensible:

<itemize>

<item> Include as little code as possible in the module trailer.  This
code is included verbatim in the generated parser, so if any of it can
go in a separate module, do so.

<item> Give type signatures for everything.  This is reported to improve
things by about 50%.  If there is a type signature for every single
non-terminal in the grammar, then Happy automatically generates type
signatures for most functions in the parser.

<item> Simplify the grammar as much as possible (applies to
everything, this one).

<item> GHC versions from around 2.06 onwards are reputedly better at
compiling Happy-generated parsers.

</itemize>

<sect1> Finding Type Errors
<label id="sec:finding-errors">
<p>

Finding type errors in grammar files is inherently difficult because
the code for reductions is moved around before being placed in the
parser.  We currently have no way of passing the original filename and
line numbers to the Haskell compiler, so there is no alternative but
to look at the parser and match the code to the grammar file.  An info
file (generated by the <tt/-i/ option) can be helpful here.

Type signature sometimes help by pinning down the particular error to
the place where the mistake is made, not half way down the file.  For
each production in the grammar, there's a bit of code in the generated
file that looks like this:

<tscreen><verb>
HappyAbsSyn<n> ( E )
</verb></tscreen>

where <tt/E/ is the Haskell expression from the grammar file (with
<tt/&dollar;n/ replaced by <tt/happy_var_n/).  If there is a type
signature for this production, then Happy will have taken it into
account when declaring the HappyAbsSyn datatype, and errors in <tt/E/
will be caught right here.  Of course, the error may be really caused
by incorrect use of one of the <tt/happy_var_n/ variables.

(this section will contain more info as we gain experience with creating
grammar files.  Please send us any helpful tips you find.)

<sect1> Conflict Tips
<label id="sec:conflict-tips">
<p>

Conflicts arise from ambiguities in the grammar.  That is, some input
sequences may possess more than one parse.  Shift/reduce conflicts are
benign in the sense that they are easily resolved (Happy automatically
selects the shift action, as this is usually the intended one).
Reduce/reduce conflicts are more serious.  A reduce/reduce conflict
implies that a certain sequence of tokens on the input can represent
more than one non-terminal, and the parser is uncertain as to which
reduction rule to use.  It will select the reduction rule uppermost in
the grammar file, so if you really must have a reduce/reduce conflict
you can select which rule will be used by putting it first in your
grammar file.

It is usually possible to remove conflicts from the grammar, but
sometimes this is at the expense of clarity and simplicity.  Here is a
cut-down example from the grammar of Haskell (1.2):

<tscreen><verb>
exp     : exp op exp0
        | exp0

exp0    : if exp then exp else exp
        ...
        | atom

atom    : var
        | integer
        | '(' exp ')'
        ...
</verb></tscreen>

This grammar has a shift/reduce conflict, due to the following
ambiguity.  In an input such as

<tscreen><verb>
if 1 then 2 else 3 + 4
</verb></tscreen>

the grammar doesn't specify whether the parse should be

<tscreen><verb>
if 1 then 2 else (3 + 4)
</verb></tscreen>

or

<tscreen><verb>
(if 1 then 2 else 3) + 4
</verb></tscreen>

and the ambiguity shows up as a shift/reduce conflict on reading the
'op' symbol.  In this case, the first parse is the intended one (the
'longest parse' rule), which corresponds to the shift action.
Removing this conflict relies on noticing that the expression on the
left-hand side of an infix operator can't be an <tt/exp0/ (the grammar
previously said otherwise, but since the conflict was resolved as
shift, this parse was not allowed).  We can reformulate the <tt/exp/
rule as:

<tscreen><verb>
exp     : atom op exp
        | exp0
</verb></tscreen>

and this removes the conflict, but at the expense of some stack space
while parsing (we turned a left-recursion into a right-recursion).
There are alternatives using left-recursion, but they all involve adding
extra states to the parser, so most programmers will prefer to keep the
conflict in favour of a clearer and more efficient parser.

<sect2> LALR(1) parsers
<label id="sec:lalr">
<p>

There are three basic ways to build a shift-reduce parser.  Full LR(1)
(the `L' is the direction in which the input is scanned, the `R' is the
way in which the parse is built, and the `1' is the number of tokens of
lookahead) generates a parser with many states, and is therefore large
and slow.  SLR(1) (simple LR(1)) is a cut-down version of LR(1) which
generates parsers with roughly one-tenth as many states, but lacks the
power to parse many grammars (it finds conflicts in grammars which have
none under LR(1)). 

LALR(1) (look-ahead LR(1)), the method used by Happy and <tt/yacc/, is
tradeoff between the two.  An LALR(1) parser has the same number of
states as an SLR(1) parser, but it uses a more complex method to
calculate the lookahead tokens that are valid at each point, and
resolves many of the conflicts that SLR(1) finds.  However, there may
still be conflicts in an LALR(1) parser that wouldn't be there with
full LR(1).

</article>
